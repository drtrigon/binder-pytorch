{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d121501-33bc-43b7-8b03-0496cb3de107",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "# https://medium.com/@gpj/predict-next-number-using-pytorch-47187c1b8e33\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the model\n",
    "class NextNumberPredictor(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, output_size):\n",
    "    super(NextNumberPredictor, self).__init__()\n",
    "    self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "    self.fc = nn.Linear(hidden_size, output_size)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    x, _ = self.lstm(x)\n",
    "    x = self.fc(x)\n",
    "    return x\n",
    "\n",
    "  # Train the model\n",
    "  def train(model, data, loss_fn, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "      for input_sequence, target in data:\n",
    "        input_sequence = torch.Tensor(input_sequence).view(len(input_sequence), 1, -1)\n",
    "        target = torch.Tensor(target).view(len(target), -1)\n",
    "      \n",
    "        # Forward pass\n",
    "        output = model(input_sequence)\n",
    "        loss = loss_fn(output, target)\n",
    "      \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "  # Test the model\n",
    "  def test(model, data, loss_fn):\n",
    "    total_loss = 0\n",
    "    for input_sequence, target in data:\n",
    "      input_sequence = torch.Tensor(input_sequence).view(len(input_sequence), 1, -1)\n",
    "      target = torch.Tensor(target).view(len(target), -1)\n",
    "      output = model(input_sequence)\n",
    "      total_loss += loss_fn(output, target).item()\n",
    "    return total_loss / len(data)\n",
    "\n",
    "# Setup the model, data, loss function and optimizer\n",
    "model = NextNumberPredictor(1, 32, 1)\n",
    "data = [(list(range(10)), list(range(1, 11))), (list(range(10, 20)), list(range(11, 21)))]\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Train the model\n",
    "model.train(data, loss_fn, optimizer, num_epochs=100)\n",
    "\n",
    "# Use the model to make predictions\n",
    "input_sequence = torch.Tensor(list(range(10, 20))).view(10, 1, -1)\n",
    "output = model(input_sequence)\n",
    "prediction = output[-1].item()\n",
    "print(f'Predicted next number: {prediction:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d74a613-a2f5-4840-a28b-42aa23ed1346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2b1cb7b-8051-495e-b9a8-2f60bbd6254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@gpj/predict-next-number-using-pytorch-47187c1b8e33\n",
    "\n",
    "# https://github.com/pytorch/examples/tree/main/time_sequence_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a2d0af0-97fc-4af1-9f98-079e8365b853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e836d01-b7ae-4059-adae-77458fd98b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1000)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "T = 20\n",
    "L = 1000\n",
    "N = 100\n",
    "\n",
    "x = np.empty((N, L), 'int64')\n",
    "x[:] = np.array(range(L)) + np.random.randint(-4 * T, 4 * T, N).reshape(N, 1)\n",
    "data = np.sin(x / 1.0 / T).astype('float64')\n",
    "torch.save(data, open('traindata.pt', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "626cead3-0971-446f-a31f-8a467ac54198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:  0\n",
      "loss: 0.5023738122475573\n",
      "loss: 0.49856639379435636\n",
      "loss: 0.479011960611529\n",
      "loss: 0.4463349021484229\n",
      "loss: 0.35406310257492946\n",
      "loss: 0.205070166176814\n",
      "loss: 1.3960531561167868\n",
      "loss: 0.03249441148471836\n",
      "loss: 0.0299348758396043\n",
      "loss: 0.028326821011534196\n",
      "loss: 0.026830612218823213\n",
      "loss: 0.023771201989989825\n",
      "loss: 0.01890141350454591\n",
      "loss: 0.010646818233205837\n",
      "loss: 0.008725752090268492\n",
      "loss: 0.007872181287777192\n",
      "loss: 0.00547784274959428\n",
      "loss: 0.004051933564063729\n",
      "loss: 0.0027296227011592606\n",
      "loss: 0.001540265276981073\n",
      "test loss: 0.0013000876156956857\n",
      "STEP:  1\n",
      "loss: 0.0012797646167828115\n",
      "loss: 0.0011690554954718023\n",
      "loss: 0.0011498916013728743\n",
      "loss: 0.0011288245291281654\n",
      "loss: 0.001063056134102438\n",
      "loss: 0.0009564006443280232\n",
      "loss: 0.0008210827589980865\n",
      "loss: 0.0007670796029147313\n",
      "loss: 0.0007294716423579147\n",
      "loss: 0.0007246558169812476\n",
      "loss: 0.0007206211393649988\n",
      "loss: 0.0007126686053108176\n",
      "loss: 0.0006961310498771502\n",
      "loss: 0.0006641101811544719\n",
      "loss: 0.0006101210702280004\n",
      "loss: 0.0005285228387106699\n",
      "loss: 0.0004127250710286698\n",
      "loss: 0.0003302157945219847\n",
      "loss: 0.0003121712149396669\n",
      "loss: 0.00032352192205803665\n",
      "test loss: 0.00017005112895092652\n",
      "STEP:  2\n",
      "loss: 0.00030530789272769515\n",
      "loss: 0.0003041496583645028\n",
      "loss: 0.00030351297478215954\n",
      "loss: 0.00030276531287189227\n",
      "loss: 0.00030103213900649724\n",
      "loss: 0.0002974408442643691\n",
      "loss: 0.00029076089116418394\n",
      "loss: 0.00028094788597888176\n",
      "loss: 0.0002682227137281097\n",
      "loss: 0.0002522191969566292\n",
      "loss: 0.0002392022232784236\n",
      "loss: 0.0002259052589346453\n",
      "loss: 0.00022719613064737113\n",
      "loss: 0.00020802021407533434\n",
      "loss: 0.00020050239660770528\n",
      "loss: 0.00019149492309973937\n",
      "loss: 0.00018777715169119417\n",
      "loss: 0.00018122790437716428\n",
      "loss: 0.00017671387788750838\n",
      "loss: 0.00016960355095444043\n",
      "test loss: 6.615079321941674e-05\n",
      "STEP:  3\n",
      "loss: 0.0001600138819592506\n",
      "loss: 0.0001537683850089298\n",
      "loss: 0.00015156060442529263\n",
      "loss: 0.00015186142350314088\n",
      "loss: 0.00015115942793885855\n",
      "loss: 0.000151028842141335\n",
      "loss: 0.00015083292549075713\n",
      "loss: 0.00015070632193117515\n",
      "loss: 0.00015045822238367063\n",
      "loss: 0.00015005142517759724\n",
      "loss: 0.0001497251329159024\n",
      "loss: 0.00014902970115520056\n",
      "loss: 0.00014791091920053612\n",
      "loss: 0.00014592474702960725\n",
      "loss: 0.00014239608685125843\n",
      "loss: 0.00013775045446543505\n",
      "loss: 0.00012341359977759226\n",
      "loss: 0.00010522420829360705\n",
      "loss: 0.00012598328784227226\n",
      "loss: 0.0001058409916926689\n",
      "test loss: 4.079810614583254e-05\n",
      "STEP:  4\n",
      "loss: 9.140770748839578e-05\n",
      "loss: 8.852362830080883e-05\n",
      "loss: 8.404062067965513e-05\n",
      "loss: 7.908645056736575e-05\n",
      "loss: 7.640384573437688e-05\n",
      "loss: 7.338813367099781e-05\n",
      "loss: 7.153854429704608e-05\n",
      "loss: 7.106764224949395e-05\n",
      "loss: 7.068615435666553e-05\n",
      "loss: 7.055590793583764e-05\n",
      "loss: 7.051954016898896e-05\n",
      "loss: 7.0481905378467e-05\n",
      "loss: 7.01675992024119e-05\n",
      "loss: 6.95917181964835e-05\n",
      "loss: 6.871306604720562e-05\n",
      "loss: 6.760283691394859e-05\n",
      "loss: 6.273061880676855e-05\n",
      "loss: 5.785888174942592e-05\n",
      "loss: 0.00019914395020923057\n",
      "loss: 5.6143339270906346e-05\n",
      "test loss: 7.010050784533406e-05\n",
      "STEP:  5\n",
      "loss: 7.675393646893812e-05\n",
      "loss: 4.299627421417643e-05\n",
      "loss: 3.680747916211949e-05\n",
      "loss: 3.453866096309384e-05\n",
      "loss: 2.9620160600808576e-05\n",
      "loss: 2.8455683809295762e-05\n",
      "loss: 2.71451038717132e-05\n",
      "loss: 2.513608968896672e-05\n",
      "loss: 2.4122530562740355e-05\n",
      "loss: 2.390323179782529e-05\n",
      "loss: 2.376741539944517e-05\n",
      "loss: 2.3747614150823694e-05\n",
      "loss: 2.3705997129721505e-05\n",
      "loss: 2.3665326596699888e-05\n",
      "loss: 2.3593522264222875e-05\n",
      "loss: 2.340702723173905e-05\n",
      "loss: 2.3087308173091674e-05\n",
      "loss: 2.286682921437433e-05\n",
      "loss: 2.257031088599748e-05\n",
      "loss: 2.1623690439678235e-05\n",
      "test loss: 2.3607026069071963e-05\n",
      "STEP:  6\n",
      "loss: 1.8973207052912707e-05\n",
      "loss: 1.7622393322256182e-05\n",
      "loss: 1.708092585681557e-05\n",
      "loss: 1.6773809816540737e-05\n",
      "loss: 1.6701357587182736e-05\n",
      "loss: 1.6669311201855537e-05\n",
      "loss: 1.665260513830599e-05\n",
      "loss: 1.6629169323136888e-05\n",
      "loss: 1.6577211064093422e-05\n",
      "loss: 1.641828305156797e-05\n",
      "loss: 1.60979868110843e-05\n",
      "loss: 1.5452587415545944e-05\n",
      "loss: 1.4400011111914194e-05\n",
      "loss: 1.4187386927461913e-05\n",
      "loss: 1.4576880532064145e-05\n",
      "loss: 1.3233679969745066e-05\n",
      "loss: 1.3074113973607464e-05\n",
      "loss: 1.2867375162943865e-05\n",
      "loss: 1.255009665564821e-05\n",
      "loss: 1.2461756450838885e-05\n",
      "test loss: 1.3238399714754586e-05\n",
      "STEP:  7\n",
      "loss: 1.2408468866034743e-05\n",
      "loss: 1.2268164902342856e-05\n",
      "loss: 1.211924646497202e-05\n",
      "loss: 1.1961571494483228e-05\n",
      "loss: 1.1909552706544287e-05\n",
      "loss: 1.1767528198926301e-05\n",
      "loss: 1.172946418946742e-05\n",
      "loss: 1.1668408095222789e-05\n",
      "loss: 1.1580505970344133e-05\n",
      "loss: 1.1412651704001122e-05\n",
      "loss: 1.1153195105475505e-05\n",
      "loss: 1.0788379889443185e-05\n",
      "loss: 1.0310705171135483e-05\n",
      "loss: 1.0667349808980666e-05\n",
      "loss: 9.40795641428122e-06\n",
      "loss: 9.01813045452765e-06\n",
      "loss: 7.988076587973058e-06\n",
      "loss: 7.562223301827642e-06\n",
      "loss: 7.287898591494384e-06\n",
      "loss: 7.1413201661218095e-06\n",
      "test loss: 9.678373790915986e-06\n",
      "STEP:  8\n",
      "loss: 7.03827205090813e-06\n",
      "loss: 6.9913502977992104e-06\n",
      "loss: 6.9474858355483305e-06\n",
      "loss: 6.919808417213321e-06\n",
      "loss: 6.879706121436619e-06\n",
      "loss: 6.8457755230822545e-06\n",
      "loss: 6.8029998874812045e-06\n",
      "loss: 6.749358145404321e-06\n",
      "loss: 6.695840552331749e-06\n",
      "loss: 6.721214568635581e-06\n",
      "loss: 6.655987620756925e-06\n",
      "loss: 6.644867603741288e-06\n",
      "loss: 6.624312898631887e-06\n",
      "loss: 6.6139414881586815e-06\n",
      "loss: 6.602866342951985e-06\n",
      "loss: 6.598236245107339e-06\n",
      "loss: 6.593677188937425e-06\n",
      "loss: 6.587785617422709e-06\n",
      "loss: 6.578857431479624e-06\n",
      "loss: 6.56366024169427e-06\n",
      "test loss: 8.549733044104845e-06\n",
      "STEP:  9\n",
      "loss: 6.539098397049378e-06\n",
      "loss: 6.502302441794913e-06\n",
      "loss: 6.447872744483557e-06\n",
      "loss: 6.375672620813487e-06\n",
      "loss: 6.26990412442622e-06\n",
      "loss: 6.064622791008205e-06\n",
      "loss: 5.784462744081555e-06\n",
      "loss: 2.6511722132922782e-05\n",
      "loss: 5.704445938372503e-06\n",
      "loss: 5.535975376867186e-06\n",
      "loss: 5.4264747893564e-06\n",
      "loss: 5.381111920373536e-06\n",
      "loss: 5.336642923789996e-06\n",
      "loss: 5.291407842044992e-06\n",
      "loss: 5.267532798741845e-06\n",
      "loss: 5.258346180301837e-06\n",
      "loss: 5.2504362324778866e-06\n",
      "loss: 5.240092109005064e-06\n",
      "loss: 5.231327906576598e-06\n",
      "loss: 5.2254791109580375e-06\n",
      "test loss: 8.3265664851321e-06\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Sequence(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Sequence, self).__init__()\n",
    "        self.lstm1 = nn.LSTMCell(1, 51)\n",
    "        self.lstm2 = nn.LSTMCell(51, 51)\n",
    "        self.linear = nn.Linear(51, 1)\n",
    "\n",
    "    def forward(self, input, future = 0):\n",
    "        outputs = []\n",
    "        h_t = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
    "        c_t = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
    "        h_t2 = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
    "        c_t2 = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
    "\n",
    "        for input_t in input.split(1, dim=1):\n",
    "            h_t, c_t = self.lstm1(input_t, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
    "            output = self.linear(h_t2)\n",
    "            outputs += [output]\n",
    "        for i in range(future):# if we should predict the future\n",
    "            h_t, c_t = self.lstm1(output, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
    "            output = self.linear(h_t2)\n",
    "            outputs += [output]\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--steps', type=int, default=15, help='steps to run')\n",
    "#    opt = parser.parse_args()\n",
    "    STEPS = 10\n",
    "    # set random seed to 0\n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    # load data and make training set\n",
    "    data = torch.load('traindata.pt')\n",
    "    input = torch.from_numpy(data[3:, :-1])\n",
    "    target = torch.from_numpy(data[3:, 1:])\n",
    "    test_input = torch.from_numpy(data[:3, :-1])\n",
    "    test_target = torch.from_numpy(data[:3, 1:])\n",
    "    # build the model\n",
    "    seq = Sequence()\n",
    "    seq.double()\n",
    "    criterion = nn.MSELoss()\n",
    "    # use LBFGS as optimizer since we can load the whole data to train\n",
    "    optimizer = optim.LBFGS(seq.parameters(), lr=0.8)\n",
    "    #begin to train\n",
    "#    for i in range(opt.steps):\n",
    "    for i in range(STEPS):\n",
    "        print('STEP: ', i)\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            out = seq(input)\n",
    "            loss = criterion(out, target)\n",
    "            print('loss:', loss.item())\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        optimizer.step(closure)\n",
    "        # begin to predict, no need to track gradient here\n",
    "        with torch.no_grad():\n",
    "            future = 1000\n",
    "            pred = seq(test_input, future=future)\n",
    "            loss = criterion(pred[:, :-future], test_target)\n",
    "            print('test loss:', loss.item())\n",
    "            y = pred.detach().numpy()\n",
    "        # draw the result\n",
    "        plt.figure(figsize=(30,10))\n",
    "        plt.title('Predict future values for time sequences\\n(Dashlines are predicted values)', fontsize=30)\n",
    "        plt.xlabel('x', fontsize=20)\n",
    "        plt.ylabel('y', fontsize=20)\n",
    "        plt.xticks(fontsize=20)\n",
    "        plt.yticks(fontsize=20)\n",
    "        def draw(yi, color):\n",
    "            plt.plot(np.arange(input.size(1)), yi[:input.size(1)], color, linewidth = 2.0)\n",
    "            plt.plot(np.arange(input.size(1), input.size(1) + future), yi[input.size(1):], color + ':', linewidth = 2.0)\n",
    "        draw(y[0], 'r')\n",
    "        draw(y[1], 'g')\n",
    "        draw(y[2], 'b')\n",
    "        plt.savefig('predict%d.pdf'%i)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790bdbb9-8bef-496f-8f82-99fa48775f31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "764be401-ac95-4091-9b5a-30ac6eee7948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4c3d128-5607-4340-8057-1e5b63249169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# globale Parameter; Datei, Zeilen und Spalten mit Informationen\n",
    "\n",
    "# Excel Datei\n",
    "filename = \"/media/sf_Dokumente/_TODO_OFFEN/Experte BMS Chur/daten_anonym.xlsx\"\n",
    "\n",
    "# Spalten mit Aufgaben\n",
    "ci = openpyxl.utils.cell.column_index_from_string\n",
    "spalten = list(range(ci('A'), ci('G')+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b28673ae-a1b3-44af-8dcb-b42fdb8492f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter; Datei, Zeilen und Spalten mit Informationen\n",
    "\n",
    "# Excel Datei\n",
    "sheet_01 = openpyxl.load_workbook(filename, data_only=True).active\n",
    "#sheet_01 = openpyxl.load_workbook(filename, data_only=True)['TBM7 4A']\n",
    "\n",
    "# Zeilen mit SuS\n",
    "zeilen_01 = list(range(4, 215+1))\n",
    "\n",
    "# Spalten mit Aufgaben\n",
    "spalten_01 = spalten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68968737-22d2-4f1e-916e-5d62603a36e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(212, 7)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Daten aus Excel Tabelle einlesen\n",
    "namen_01 = []\n",
    "punkte_01 = []\n",
    "noten_01 = []\n",
    "pnoten_01 = []\n",
    "enoten_01 = []\n",
    "fnoten_01 = []\n",
    "for z in zeilen_01:\n",
    "    p = []\n",
    "    for s in spalten_01:\n",
    "        p.append( sheet_01.cell(z, s).value )\n",
    "    noten_01.append( p )\n",
    "\n",
    "np.array( noten_01 ).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3dfba6da-d5a6-46cd-be13-fb4d278313c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[[5.5, 4.5, 5, 5, null, 5, 5.5], [3, 3, 4, 4, null, 3.5, 3.5], [5.5, 6, 6, 5.5, null, 6, 6], [4.5, 4, 4.5, 5, null, 4.5, 4.5], [2.5, 3, 4, 2.5, null, 3, 3], [2.5, 3, 3.5, 3, null, 3, 3], [3.5, 4, 4, 4.5, null, 4, 4], [4.5, 4, 4, 4.5, null, 4, 4.5], [4, 4.5, 4.5, 4, null, 4.5, 4.5], [3, 3, 3.5, 3.5, null, 3.5, 3.5], [5.5, 6, 6, 6, null, 6, 6], [4, 4.5, 5, 5, null, 5, 4.5], [5, 3.5, 4, 4.5, null, 4, 4.5], [4, 3.5, 4.5, 4, null, 4, 4], [4, 4, 4, 3.5, 4, 4, 4], [4.5, 4.5, 5.5, 4.5, 4.5, 5, 5], [1.5, 2.5, 3.5, 3, 2, 3, 2.5], [null, 3, 3.5, 2.5, 2.5, 3, 3], [4.5, 5, 5, 5, 5, 5, 5], [3.5, 4.5, 4.5, 4, 4.5, 4.5, 4], [4, 5.5, 5.5, 4.5, 5.5, 5.5, 5], [4.5, 4, 4, 3.5, 3.5, 4, 4.5], [2.5, 4, 4, 4, 4, 4, 3.5], [5, 5, 5, 5, 4.5, 5, 5], [6, 5.5, 5.5, 4.5, 6, 5.5, 6], [4.5, 4.5, 4.5, 3, 5, 4.5, 4.5], [4, 5, 5, 4, 5, 5, 4.5], [4, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5], [3, 4, 4, 2.5, 3, 3.5, 3.5], [2, 3.5, 3.5, 2.5, 2.5, 3, 2.5], [3, 4, 4, 3.5, 3.5, 4, 3.5], [2, 4.5, 4.5, 3, 4, 4, 3], [2.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3], [4, 4, 4.5, 3.5, 3.5, 4, 4], [4, 4.5, 4.5, 4, 5.5, 4.5, 4.5], [4, 5.5, 5.5, 5.5, 4.5, 5.5, 5], [3.5, 4, 4, 4, 4, 4, 4], [3.5, 4, 4.5, 4.5, 4, 4.5, 4], [3, 3.5, 4, 3.5, 3.5, 3.5, 3.5], [2.5, 4, 3, null, null, 3.5, 3], [3.5, 4.5, 4.5, null, null, 4.5, 4], [2.5, 5, 3, null, null, 4, 3.5], [5, 5.5, 5, null, null, 5.5, 5.5], [3.5, 4.5, 4.5, null, null, 4.5, 4], [2, 3.5, 3, null, null, 3.5, 3], [3.5, 4.5, 4.5, null, null, 4.5, 4], [4, 5, 4.5, null, null, 5, 4.5], [3.5, 4.5, 3.5, null, null, 4, 4], [4.5, 4.5, 4.5, null, null, 4.5, 4.5], [4, 4.5, 4.5, null, null, 4.5, 4.5], [3.5, 4, 3.5, null, null, 4, 4], [4, 5, 5, null, null, 5, 4.5], [4, 5.5, 5.5, null, null, 5.5, 5], [4, 3.5, 4, null, null, 4, 4], [3.5, 4, 4, null, null, 4, 4], [4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5], [5.5, 5, 5, 5.5, 4.5, 5, 5.5], [5.5, 5.5, 4.5, 5, 4.5, 5, 5.5], [4.5, 4, 4.5, 3.5, 4, 4, 4.5], [5.5, 5, 5, 5.5, 5, 5, 5.5], [6, 5, 5.5, 5, 4.5, 5, 5.5], [5, 5.5, 5.5, 5.5, 5, 5.5, 5.5], [5, 4.5, 5, 5, 4.5, 5, 5], [4, 4.5, 5, 4.5, 4, 4.5, 4.5], [4, 4, 4, 3.5, 3.5, 4, 4], [3, 4.5, 4, 4, 4, 4, 3.5], [4.5, 5, 5.5, 4.5, 3.5, 4.5, 4.5], [6, 5.5, 5, 5.5, 5, 5.5, 6], [6, 5.5, 5, 5.5, 5, 5.5, 6], [4, 4, 3, 3, 3, 3.5, 4], [4, 4, 4, 4.5, 3.5, 4, 4], [5, 4, 4, 4.5, 4.5, 4.5, 5], [4, 4.5, 4.5, 4, 3.5, 4, 4], [4, 4.5, 3.5, 3.5, 3.5, 4, 4], [4.5, 4, 4, 3.5, 4.5, 4, 4.5], [5, 4, 3, 4.5, 4, 4, 4.5], [3.5, 4.5, 4.5, 4, 3.5, 4, 4], [3, 4.5, 4.5, 4.5, 3.5, 4.5, 4], [4, 3.5, 4, 3.5, 4, 4, 4], [3.5, 4, 4, 4, 4, 4, 4], [3.5, 5, 4, 4, 4, 4.5, 4], [4, 4.5, 4, 4.5, 4, 4.5, 4.5], [4, 5, 4.5, 5.5, 4, 5, 4.5], [4, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5], [3.5, 4.5, 3.5, 4, 4, 4, 4], [4, 4.5, 4, 4, 4.5, 4.5, 4.5], [3.5, 4.5, 4, 4.5, 4, 4.5, 4], [3.5, 4, 3.5, 3, 3.5, 3.5, 3.5], [3, 3, 3.5, 3, 4, 3.5, 3.5], [4.5, 5, 5.5, 5.5, 6, 5.5, 5], [5, 4, 4.5, 5.5, 3.5, 4.5, 5], [3.5, 3.5, 3.5, 4, 3.5, 3.5, 3.5], [5, 5, 5.5, 4.5, 4, 5, 5], [4.5, 4.5, 4, 4.5, 4, 4.5, 4.5], [5, 4.5, 5, 4.5, 4.5, 4.5, 5], [4.5, 5, 4.5, 4.5, 5, 5, 5], [3, 4, 3.5, 3.5, 3, 3.5, 3.5], [4, 4.5, 3.5, 3.5, 3.5, 4, 4], [1.5, 2.5, 2.5, 2, 1.5, 2, 2], [3.5, 5, 4.5, 4.5, 5, 5, 4.5], [5, 5, 5, 5, 4.5, 5, 5], [4.5, 4.5, 3.5, 5, 5, 4.5, 4.5], [3, 4.5, 4, null, null, 4.5, 4], [3, 4.5, 4, null, null, 4.5, 4], [4, 5, 4.5, null, null, 5, 4.5], [4.5, 4.5, 4.5, null, null, 4.5, 4.5], [4.5, 5.5, 5, null, null, 5.5, 5], [5, 5.5, 5, null, null, 5.5, 5.5], [1.5, 3, 2, null, null, 2.5, 2], [5, 4.5, 4, null, null, 4.5, 5], [3.5, 3, 3.5, null, null, 3.5, 3.5], [5.5, 5, 4.5, null, null, 5, 5.5], [4.5, 4.5, 4.5, null, null, 4.5, 4.5], [2.5, 3.5, 3, null, null, 3.5, 3], [4, 4.5, 4, null, null, 4.5, 4.5], [6, 4.5, 4.5, null, null, 4.5, 5.5], [4, 4, 4, null, null, 4, 4], [4.5, 4, 4, null, null, 4, 4.5], [4.5, 4.5, 4.5, null, null, 4.5, 4.5], [5, 4, 4.5, null, null, 4.5, 5], [5.5, 5, 5, null, null, 5, 5.5], [5.5, 4.5, 5, null, null, 5, 5.5], [3.5, 3.5, 3.5, null, null, 3.5, 3.5], [5, 4, 4, null, null, 4, 4.5], [5.5, 4, 5.5, 5, 5, 5, 5.5], [4, 4.5, 4, 3.5, 3.5, 4, 4], [3, 4.5, 4, 4, 3.5, 4, 3.5], [4.5, 5, 3.5, 4, 5, 4.5, 4.5], [4, 4.5, 4, 4, 4.5, 4.5, 4.5], [5, 3.5, 5, 5, 4.5, 4.5, 5], [4, 4, 5, 4, 4, 4.5, 4.5], [4, 3, 4, 3, 4, 3.5, 4], [4.5, 4.5, 4, 4, 4.5, 4.5, 4.5], [3.5, 3.5, 2.5, 3.5, 2.5, 3, 3.5], [4.5, 3, 4, 3.5, 5, 4, 4.5], [3, 3.5, 2.5, 3, 4, 3.5, 3.5], [3.5, 4.5, 4.5, 4, 5.5, 4.5, 4], [4, 3.5, 3.5, 4, 4.5, 4, 4], [3, 4.5, 4.5, 4, 3, 4, 3.5], [4.5, 4, 4.5, 4.5, 5, 4.5, 4.5], [4.5, 4, 4, 5, 4.5, 4.5, 4.5], [4, 4, 4, 4, 5.5, 4.5, 4.5], [4.5, 4, 4, 4, 5, 4.5, 4.5], [4, 4.5, 4.5, 4.5, 5, 4.5, 4.5], [5, 5, 4.5, 4, 4.5, 4.5, 5], [3.5, 4.5, 4, 3.5, 4, 4, 4], [5, 4, 3, 4, 4.5, 4, 4.5], [4, 4, 4, 4, 4, 4, 4], [4, 4, 4.5, 4, 4, 4, 4], [2.5, 5, 4.5, 4, 4, 4.5, 3.5], [5, 5, 5, 4, 4.5, 4.5, 5], [4, 5.5, 4.5, 5, 4.5, 5, 4.5], [2, 4, 3.5, 3, 3, 3.5, 3], [5.5, 6, 6, 5.5, 5.5, 6, 6], [3, 3.5, 3, null, null, 3.5, 3.5], [3, 3.5, 3, null, null, 3.5, 3.5], [3, 4, 3.5, null, null, 4, 3.5], [2.5, 3.5, 3.5, null, null, 3.5, 3], [2, 2.5, 2, null, null, 2.5, 2.5], [1.5, 2.5, 2, null, null, 2.5, 2], [2.5, 4, 3.5, null, null, 4, 3.5], [3.5, 4.5, 3.5, null, null, 4, 4], [3.5, 4.5, 3.5, null, null, 4, 4], [2.5, 3.5, 4, null, null, 4, 3.5], [4, 5.5, 5, null, null, 5.5, 5], [5, 5, 5, null, null, 5, 5], [5, 5, 5, 5, 5, 5, 5], [5, 4.5, 4.5, 4.5, 4.5, 4.5, 5], [3.5, 4, 4, 4.5, 4, 4, 4], [5, 4.5, 4, 4.5, 4, 4.5, 5], [3.5, 4, 4, 3.5, 4, 4, 4], [4, null, 5, 4.5, 4.5, 4.5, 4.5], [2.5, 4.5, 4.5, 4.5, 3.5, 4.5, 3.5], [3.5, 4.5, 5, 4, 5, 4.5, 4], [4.5, 5, 5.5, 4.5, 5, 5, 5], [5, 4.5, 5, 5, 5.5, 5, 5], [5, 4, 4.5, 4.5, 4.5, 4.5, 5], [4.5, 4, 4.5, 4.5, 4, 4.5, 4.5], [2.5, 3.5, 3.5, 3.5, 3, 3.5, 3], [3.5, 4.5, 4, 3.5, 3.5, 4, 4], [5.5, 5, 5.5, 5.5, 5.5, 5.5, 5.5], [2.5, 4, 4, 4, 4.5, 4, 3.5], [4, 3.5, 3, 2.5, 3.5, 3, 3.5], [5, 5.5, 5.5, 5.5, 3.5, 5, 5], [4.5, 4, 4, 3.5, 5, 4, 4.5], [4.5, 5, 5, 5.5, 5, 5, 5], [4, 4, 4.5, 4.5, 3.5, 4, 4], [3, 5, 3, 3.5, 3.5, 4, 3.5], [4.5, 5.5, 5.5, 5, 5, 5.5, 5], [6, 6, 6, 6, 5.5, 6, 6], [3.5, 3.5, 3, 3, 3, 3, 3.5], [4, 4, 4.5, 4, 4, 4, 4], [4.5, 4.5, 5, 4.5, 4.5, 4.5, 4.5], [4, 3, 4, 4, 4.5, 4, 4], [3.5, 4.5, 4, 3.5, 3.5, 4, 4], [2.5, 2.5, 3, 2.5, 3, 3, 3], [4, 4, 5, 4, 4, 4.5, 4.5], [4.5, 3.5, 5, 4.5, 4, 4.5, 4.5], [5, 5, 4.5, 4, 3.5, 4.5, 5], [4.5, 4, 4, 3.5, 3.5, 4, 4.5], [4, 3.5, 4, 4, 3.5, 4, 4], [4, 4, 4.5, 4.5, 4.5, 4.5, 4.5], [4.5, 4.5, 4, 4, 4.5, 4.5, 4.5], [4.5, 4.5, 5, 4.5, 3.5, 4.5, 4.5], [3.5, 4.5, 5, null, null, 5, 4.5], [2.5, 4, 4, null, null, 4, 3.5], [5, 4.5, 4.5, null, null, 4.5, 5], [2, 3, 3, null, null, 3, 2.5], [3.5, 4, 4, null, null, 4, 4], [3.5, 5, 4.5, null, null, 5, 4.5], [2, 4, 3.5, null, null, 4, 3], [2, 2.5, 2.5, null, null, 2.5, 2.5]]'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dumps(noten_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d61e4ca-4117-4245-9856-ba420bf6b374",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.loads('[[5.5, 4.5, 5, 5, null, 5, 5.5], [3, 3, 4, 4, null, 3.5, 3.5], [5.5, 6, 6, 5.5, null, 6, 6], [4.5, 4, 4.5, 5, null, 4.5, 4.5], [2.5, 3, 4, 2.5, null, 3, 3], [2.5, 3, 3.5, 3, null, 3, 3], [3.5, 4, 4, 4.5, null, 4, 4], [4.5, 4, 4, 4.5, null, 4, 4.5], [4, 4.5, 4.5, 4, null, 4.5, 4.5], [3, 3, 3.5, 3.5, null, 3.5, 3.5], [5.5, 6, 6, 6, null, 6, 6], [4, 4.5, 5, 5, null, 5, 4.5], [5, 3.5, 4, 4.5, null, 4, 4.5], [4, 3.5, 4.5, 4, null, 4, 4], [4, 4, 4, 3.5, 4, 4, 4], [4.5, 4.5, 5.5, 4.5, 4.5, 5, 5], [1.5, 2.5, 3.5, 3, 2, 3, 2.5], [null, 3, 3.5, 2.5, 2.5, 3, 3], [4.5, 5, 5, 5, 5, 5, 5], [3.5, 4.5, 4.5, 4, 4.5, 4.5, 4], [4, 5.5, 5.5, 4.5, 5.5, 5.5, 5], [4.5, 4, 4, 3.5, 3.5, 4, 4.5], [2.5, 4, 4, 4, 4, 4, 3.5], [5, 5, 5, 5, 4.5, 5, 5], [6, 5.5, 5.5, 4.5, 6, 5.5, 6], [4.5, 4.5, 4.5, 3, 5, 4.5, 4.5], [4, 5, 5, 4, 5, 5, 4.5], [4, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5], [3, 4, 4, 2.5, 3, 3.5, 3.5], [2, 3.5, 3.5, 2.5, 2.5, 3, 2.5], [3, 4, 4, 3.5, 3.5, 4, 3.5], [2, 4.5, 4.5, 3, 4, 4, 3], [2.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3], [4, 4, 4.5, 3.5, 3.5, 4, 4], [4, 4.5, 4.5, 4, 5.5, 4.5, 4.5], [4, 5.5, 5.5, 5.5, 4.5, 5.5, 5], [3.5, 4, 4, 4, 4, 4, 4], [3.5, 4, 4.5, 4.5, 4, 4.5, 4], [3, 3.5, 4, 3.5, 3.5, 3.5, 3.5], [2.5, 4, 3, null, null, 3.5, 3], [3.5, 4.5, 4.5, null, null, 4.5, 4], [2.5, 5, 3, null, null, 4, 3.5], [5, 5.5, 5, null, null, 5.5, 5.5], [3.5, 4.5, 4.5, null, null, 4.5, 4], [2, 3.5, 3, null, null, 3.5, 3], [3.5, 4.5, 4.5, null, null, 4.5, 4], [4, 5, 4.5, null, null, 5, 4.5], [3.5, 4.5, 3.5, null, null, 4, 4], [4.5, 4.5, 4.5, null, null, 4.5, 4.5], [4, 4.5, 4.5, null, null, 4.5, 4.5], [3.5, 4, 3.5, null, null, 4, 4], [4, 5, 5, null, null, 5, 4.5], [4, 5.5, 5.5, null, null, 5.5, 5], [4, 3.5, 4, null, null, 4, 4], [3.5, 4, 4, null, null, 4, 4], [4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5], [5.5, 5, 5, 5.5, 4.5, 5, 5.5], [5.5, 5.5, 4.5, 5, 4.5, 5, 5.5], [4.5, 4, 4.5, 3.5, 4, 4, 4.5], [5.5, 5, 5, 5.5, 5, 5, 5.5], [6, 5, 5.5, 5, 4.5, 5, 5.5], [5, 5.5, 5.5, 5.5, 5, 5.5, 5.5], [5, 4.5, 5, 5, 4.5, 5, 5], [4, 4.5, 5, 4.5, 4, 4.5, 4.5], [4, 4, 4, 3.5, 3.5, 4, 4], [3, 4.5, 4, 4, 4, 4, 3.5], [4.5, 5, 5.5, 4.5, 3.5, 4.5, 4.5], [6, 5.5, 5, 5.5, 5, 5.5, 6], [6, 5.5, 5, 5.5, 5, 5.5, 6], [4, 4, 3, 3, 3, 3.5, 4], [4, 4, 4, 4.5, 3.5, 4, 4], [5, 4, 4, 4.5, 4.5, 4.5, 5], [4, 4.5, 4.5, 4, 3.5, 4, 4], [4, 4.5, 3.5, 3.5, 3.5, 4, 4], [4.5, 4, 4, 3.5, 4.5, 4, 4.5], [5, 4, 3, 4.5, 4, 4, 4.5], [3.5, 4.5, 4.5, 4, 3.5, 4, 4], [3, 4.5, 4.5, 4.5, 3.5, 4.5, 4], [4, 3.5, 4, 3.5, 4, 4, 4], [3.5, 4, 4, 4, 4, 4, 4], [3.5, 5, 4, 4, 4, 4.5, 4], [4, 4.5, 4, 4.5, 4, 4.5, 4.5], [4, 5, 4.5, 5.5, 4, 5, 4.5], [4, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5], [3.5, 4.5, 3.5, 4, 4, 4, 4], [4, 4.5, 4, 4, 4.5, 4.5, 4.5], [3.5, 4.5, 4, 4.5, 4, 4.5, 4], [3.5, 4, 3.5, 3, 3.5, 3.5, 3.5], [3, 3, 3.5, 3, 4, 3.5, 3.5], [4.5, 5, 5.5, 5.5, 6, 5.5, 5], [5, 4, 4.5, 5.5, 3.5, 4.5, 5], [3.5, 3.5, 3.5, 4, 3.5, 3.5, 3.5], [5, 5, 5.5, 4.5, 4, 5, 5], [4.5, 4.5, 4, 4.5, 4, 4.5, 4.5], [5, 4.5, 5, 4.5, 4.5, 4.5, 5], [4.5, 5, 4.5, 4.5, 5, 5, 5], [3, 4, 3.5, 3.5, 3, 3.5, 3.5], [4, 4.5, 3.5, 3.5, 3.5, 4, 4], [1.5, 2.5, 2.5, 2, 1.5, 2, 2], [3.5, 5, 4.5, 4.5, 5, 5, 4.5], [5, 5, 5, 5, 4.5, 5, 5], [4.5, 4.5, 3.5, 5, 5, 4.5, 4.5], [3, 4.5, 4, null, null, 4.5, 4], [3, 4.5, 4, null, null, 4.5, 4], [4, 5, 4.5, null, null, 5, 4.5], [4.5, 4.5, 4.5, null, null, 4.5, 4.5], [4.5, 5.5, 5, null, null, 5.5, 5], [5, 5.5, 5, null, null, 5.5, 5.5], [1.5, 3, 2, null, null, 2.5, 2], [5, 4.5, 4, null, null, 4.5, 5], [3.5, 3, 3.5, null, null, 3.5, 3.5], [5.5, 5, 4.5, null, null, 5, 5.5], [4.5, 4.5, 4.5, null, null, 4.5, 4.5], [2.5, 3.5, 3, null, null, 3.5, 3], [4, 4.5, 4, null, null, 4.5, 4.5], [6, 4.5, 4.5, null, null, 4.5, 5.5], [4, 4, 4, null, null, 4, 4], [4.5, 4, 4, null, null, 4, 4.5], [4.5, 4.5, 4.5, null, null, 4.5, 4.5], [5, 4, 4.5, null, null, 4.5, 5], [5.5, 5, 5, null, null, 5, 5.5], [5.5, 4.5, 5, null, null, 5, 5.5], [3.5, 3.5, 3.5, null, null, 3.5, 3.5], [5, 4, 4, null, null, 4, 4.5], [5.5, 4, 5.5, 5, 5, 5, 5.5], [4, 4.5, 4, 3.5, 3.5, 4, 4], [3, 4.5, 4, 4, 3.5, 4, 3.5], [4.5, 5, 3.5, 4, 5, 4.5, 4.5], [4, 4.5, 4, 4, 4.5, 4.5, 4.5], [5, 3.5, 5, 5, 4.5, 4.5, 5], [4, 4, 5, 4, 4, 4.5, 4.5], [4, 3, 4, 3, 4, 3.5, 4], [4.5, 4.5, 4, 4, 4.5, 4.5, 4.5], [3.5, 3.5, 2.5, 3.5, 2.5, 3, 3.5], [4.5, 3, 4, 3.5, 5, 4, 4.5], [3, 3.5, 2.5, 3, 4, 3.5, 3.5], [3.5, 4.5, 4.5, 4, 5.5, 4.5, 4], [4, 3.5, 3.5, 4, 4.5, 4, 4], [3, 4.5, 4.5, 4, 3, 4, 3.5], [4.5, 4, 4.5, 4.5, 5, 4.5, 4.5], [4.5, 4, 4, 5, 4.5, 4.5, 4.5], [4, 4, 4, 4, 5.5, 4.5, 4.5], [4.5, 4, 4, 4, 5, 4.5, 4.5], [4, 4.5, 4.5, 4.5, 5, 4.5, 4.5], [5, 5, 4.5, 4, 4.5, 4.5, 5], [3.5, 4.5, 4, 3.5, 4, 4, 4], [5, 4, 3, 4, 4.5, 4, 4.5], [4, 4, 4, 4, 4, 4, 4], [4, 4, 4.5, 4, 4, 4, 4], [2.5, 5, 4.5, 4, 4, 4.5, 3.5], [5, 5, 5, 4, 4.5, 4.5, 5], [4, 5.5, 4.5, 5, 4.5, 5, 4.5], [2, 4, 3.5, 3, 3, 3.5, 3], [5.5, 6, 6, 5.5, 5.5, 6, 6], [3, 3.5, 3, null, null, 3.5, 3.5], [3, 3.5, 3, null, null, 3.5, 3.5], [3, 4, 3.5, null, null, 4, 3.5], [2.5, 3.5, 3.5, null, null, 3.5, 3], [2, 2.5, 2, null, null, 2.5, 2.5], [1.5, 2.5, 2, null, null, 2.5, 2], [2.5, 4, 3.5, null, null, 4, 3.5], [3.5, 4.5, 3.5, null, null, 4, 4], [3.5, 4.5, 3.5, null, null, 4, 4], [2.5, 3.5, 4, null, null, 4, 3.5], [4, 5.5, 5, null, null, 5.5, 5], [5, 5, 5, null, null, 5, 5], [5, 5, 5, 5, 5, 5, 5], [5, 4.5, 4.5, 4.5, 4.5, 4.5, 5], [3.5, 4, 4, 4.5, 4, 4, 4], [5, 4.5, 4, 4.5, 4, 4.5, 5], [3.5, 4, 4, 3.5, 4, 4, 4], [4, null, 5, 4.5, 4.5, 4.5, 4.5], [2.5, 4.5, 4.5, 4.5, 3.5, 4.5, 3.5], [3.5, 4.5, 5, 4, 5, 4.5, 4], [4.5, 5, 5.5, 4.5, 5, 5, 5], [5, 4.5, 5, 5, 5.5, 5, 5], [5, 4, 4.5, 4.5, 4.5, 4.5, 5], [4.5, 4, 4.5, 4.5, 4, 4.5, 4.5], [2.5, 3.5, 3.5, 3.5, 3, 3.5, 3], [3.5, 4.5, 4, 3.5, 3.5, 4, 4], [5.5, 5, 5.5, 5.5, 5.5, 5.5, 5.5], [2.5, 4, 4, 4, 4.5, 4, 3.5], [4, 3.5, 3, 2.5, 3.5, 3, 3.5], [5, 5.5, 5.5, 5.5, 3.5, 5, 5], [4.5, 4, 4, 3.5, 5, 4, 4.5], [4.5, 5, 5, 5.5, 5, 5, 5], [4, 4, 4.5, 4.5, 3.5, 4, 4], [3, 5, 3, 3.5, 3.5, 4, 3.5], [4.5, 5.5, 5.5, 5, 5, 5.5, 5], [6, 6, 6, 6, 5.5, 6, 6], [3.5, 3.5, 3, 3, 3, 3, 3.5], [4, 4, 4.5, 4, 4, 4, 4], [4.5, 4.5, 5, 4.5, 4.5, 4.5, 4.5], [4, 3, 4, 4, 4.5, 4, 4], [3.5, 4.5, 4, 3.5, 3.5, 4, 4], [2.5, 2.5, 3, 2.5, 3, 3, 3], [4, 4, 5, 4, 4, 4.5, 4.5], [4.5, 3.5, 5, 4.5, 4, 4.5, 4.5], [5, 5, 4.5, 4, 3.5, 4.5, 5], [4.5, 4, 4, 3.5, 3.5, 4, 4.5], [4, 3.5, 4, 4, 3.5, 4, 4], [4, 4, 4.5, 4.5, 4.5, 4.5, 4.5], [4.5, 4.5, 4, 4, 4.5, 4.5, 4.5], [4.5, 4.5, 5, 4.5, 3.5, 4.5, 4.5], [3.5, 4.5, 5, null, null, 5, 4.5], [2.5, 4, 4, null, null, 4, 3.5], [5, 4.5, 4.5, null, null, 4.5, 5], [2, 3, 3, null, null, 3, 2.5], [3.5, 4, 4, null, null, 4, 4], [3.5, 5, 4.5, null, null, 5, 4.5], [2, 4, 3.5, null, null, 4, 3], [2, 2.5, 2.5, null, null, 2.5, 2.5]]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8181cb22-6c20-4733-887e-3b282feadda7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((212, 7),\n",
       " [[5.5, 4.5, 5, 5, None, 5, 5.5],\n",
       "  [3, 3, 4, 4, None, 3.5, 3.5],\n",
       "  [5.5, 6, 6, 5.5, None, 6, 6],\n",
       "  [4.5, 4, 4.5, 5, None, 4.5, 4.5],\n",
       "  [2.5, 3, 4, 2.5, None, 3, 3],\n",
       "  [2.5, 3, 3.5, 3, None, 3, 3],\n",
       "  [3.5, 4, 4, 4.5, None, 4, 4],\n",
       "  [4.5, 4, 4, 4.5, None, 4, 4.5],\n",
       "  [4, 4.5, 4.5, 4, None, 4.5, 4.5],\n",
       "  [3, 3, 3.5, 3.5, None, 3.5, 3.5],\n",
       "  [5.5, 6, 6, 6, None, 6, 6],\n",
       "  [4, 4.5, 5, 5, None, 5, 4.5],\n",
       "  [5, 3.5, 4, 4.5, None, 4, 4.5],\n",
       "  [4, 3.5, 4.5, 4, None, 4, 4],\n",
       "  [4, 4, 4, 3.5, 4, 4, 4],\n",
       "  [4.5, 4.5, 5.5, 4.5, 4.5, 5, 5],\n",
       "  [1.5, 2.5, 3.5, 3, 2, 3, 2.5],\n",
       "  [None, 3, 3.5, 2.5, 2.5, 3, 3],\n",
       "  [4.5, 5, 5, 5, 5, 5, 5],\n",
       "  [3.5, 4.5, 4.5, 4, 4.5, 4.5, 4],\n",
       "  [4, 5.5, 5.5, 4.5, 5.5, 5.5, 5],\n",
       "  [4.5, 4, 4, 3.5, 3.5, 4, 4.5],\n",
       "  [2.5, 4, 4, 4, 4, 4, 3.5],\n",
       "  [5, 5, 5, 5, 4.5, 5, 5],\n",
       "  [6, 5.5, 5.5, 4.5, 6, 5.5, 6],\n",
       "  [4.5, 4.5, 4.5, 3, 5, 4.5, 4.5],\n",
       "  [4, 5, 5, 4, 5, 5, 4.5],\n",
       "  [4, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5],\n",
       "  [3, 4, 4, 2.5, 3, 3.5, 3.5],\n",
       "  [2, 3.5, 3.5, 2.5, 2.5, 3, 2.5],\n",
       "  [3, 4, 4, 3.5, 3.5, 4, 3.5],\n",
       "  [2, 4.5, 4.5, 3, 4, 4, 3],\n",
       "  [2.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3],\n",
       "  [4, 4, 4.5, 3.5, 3.5, 4, 4],\n",
       "  [4, 4.5, 4.5, 4, 5.5, 4.5, 4.5],\n",
       "  [4, 5.5, 5.5, 5.5, 4.5, 5.5, 5],\n",
       "  [3.5, 4, 4, 4, 4, 4, 4],\n",
       "  [3.5, 4, 4.5, 4.5, 4, 4.5, 4],\n",
       "  [3, 3.5, 4, 3.5, 3.5, 3.5, 3.5],\n",
       "  [2.5, 4, 3, None, None, 3.5, 3],\n",
       "  [3.5, 4.5, 4.5, None, None, 4.5, 4],\n",
       "  [2.5, 5, 3, None, None, 4, 3.5],\n",
       "  [5, 5.5, 5, None, None, 5.5, 5.5],\n",
       "  [3.5, 4.5, 4.5, None, None, 4.5, 4],\n",
       "  [2, 3.5, 3, None, None, 3.5, 3],\n",
       "  [3.5, 4.5, 4.5, None, None, 4.5, 4],\n",
       "  [4, 5, 4.5, None, None, 5, 4.5],\n",
       "  [3.5, 4.5, 3.5, None, None, 4, 4],\n",
       "  [4.5, 4.5, 4.5, None, None, 4.5, 4.5],\n",
       "  [4, 4.5, 4.5, None, None, 4.5, 4.5],\n",
       "  [3.5, 4, 3.5, None, None, 4, 4],\n",
       "  [4, 5, 5, None, None, 5, 4.5],\n",
       "  [4, 5.5, 5.5, None, None, 5.5, 5],\n",
       "  [4, 3.5, 4, None, None, 4, 4],\n",
       "  [3.5, 4, 4, None, None, 4, 4],\n",
       "  [4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5],\n",
       "  [5.5, 5, 5, 5.5, 4.5, 5, 5.5],\n",
       "  [5.5, 5.5, 4.5, 5, 4.5, 5, 5.5],\n",
       "  [4.5, 4, 4.5, 3.5, 4, 4, 4.5],\n",
       "  [5.5, 5, 5, 5.5, 5, 5, 5.5],\n",
       "  [6, 5, 5.5, 5, 4.5, 5, 5.5],\n",
       "  [5, 5.5, 5.5, 5.5, 5, 5.5, 5.5],\n",
       "  [5, 4.5, 5, 5, 4.5, 5, 5],\n",
       "  [4, 4.5, 5, 4.5, 4, 4.5, 4.5],\n",
       "  [4, 4, 4, 3.5, 3.5, 4, 4],\n",
       "  [3, 4.5, 4, 4, 4, 4, 3.5],\n",
       "  [4.5, 5, 5.5, 4.5, 3.5, 4.5, 4.5],\n",
       "  [6, 5.5, 5, 5.5, 5, 5.5, 6],\n",
       "  [6, 5.5, 5, 5.5, 5, 5.5, 6],\n",
       "  [4, 4, 3, 3, 3, 3.5, 4],\n",
       "  [4, 4, 4, 4.5, 3.5, 4, 4],\n",
       "  [5, 4, 4, 4.5, 4.5, 4.5, 5],\n",
       "  [4, 4.5, 4.5, 4, 3.5, 4, 4],\n",
       "  [4, 4.5, 3.5, 3.5, 3.5, 4, 4],\n",
       "  [4.5, 4, 4, 3.5, 4.5, 4, 4.5],\n",
       "  [5, 4, 3, 4.5, 4, 4, 4.5],\n",
       "  [3.5, 4.5, 4.5, 4, 3.5, 4, 4],\n",
       "  [3, 4.5, 4.5, 4.5, 3.5, 4.5, 4],\n",
       "  [4, 3.5, 4, 3.5, 4, 4, 4],\n",
       "  [3.5, 4, 4, 4, 4, 4, 4],\n",
       "  [3.5, 5, 4, 4, 4, 4.5, 4],\n",
       "  [4, 4.5, 4, 4.5, 4, 4.5, 4.5],\n",
       "  [4, 5, 4.5, 5.5, 4, 5, 4.5],\n",
       "  [4, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5],\n",
       "  [3.5, 4.5, 3.5, 4, 4, 4, 4],\n",
       "  [4, 4.5, 4, 4, 4.5, 4.5, 4.5],\n",
       "  [3.5, 4.5, 4, 4.5, 4, 4.5, 4],\n",
       "  [3.5, 4, 3.5, 3, 3.5, 3.5, 3.5],\n",
       "  [3, 3, 3.5, 3, 4, 3.5, 3.5],\n",
       "  [4.5, 5, 5.5, 5.5, 6, 5.5, 5],\n",
       "  [5, 4, 4.5, 5.5, 3.5, 4.5, 5],\n",
       "  [3.5, 3.5, 3.5, 4, 3.5, 3.5, 3.5],\n",
       "  [5, 5, 5.5, 4.5, 4, 5, 5],\n",
       "  [4.5, 4.5, 4, 4.5, 4, 4.5, 4.5],\n",
       "  [5, 4.5, 5, 4.5, 4.5, 4.5, 5],\n",
       "  [4.5, 5, 4.5, 4.5, 5, 5, 5],\n",
       "  [3, 4, 3.5, 3.5, 3, 3.5, 3.5],\n",
       "  [4, 4.5, 3.5, 3.5, 3.5, 4, 4],\n",
       "  [1.5, 2.5, 2.5, 2, 1.5, 2, 2],\n",
       "  [3.5, 5, 4.5, 4.5, 5, 5, 4.5],\n",
       "  [5, 5, 5, 5, 4.5, 5, 5],\n",
       "  [4.5, 4.5, 3.5, 5, 5, 4.5, 4.5],\n",
       "  [3, 4.5, 4, None, None, 4.5, 4],\n",
       "  [3, 4.5, 4, None, None, 4.5, 4],\n",
       "  [4, 5, 4.5, None, None, 5, 4.5],\n",
       "  [4.5, 4.5, 4.5, None, None, 4.5, 4.5],\n",
       "  [4.5, 5.5, 5, None, None, 5.5, 5],\n",
       "  [5, 5.5, 5, None, None, 5.5, 5.5],\n",
       "  [1.5, 3, 2, None, None, 2.5, 2],\n",
       "  [5, 4.5, 4, None, None, 4.5, 5],\n",
       "  [3.5, 3, 3.5, None, None, 3.5, 3.5],\n",
       "  [5.5, 5, 4.5, None, None, 5, 5.5],\n",
       "  [4.5, 4.5, 4.5, None, None, 4.5, 4.5],\n",
       "  [2.5, 3.5, 3, None, None, 3.5, 3],\n",
       "  [4, 4.5, 4, None, None, 4.5, 4.5],\n",
       "  [6, 4.5, 4.5, None, None, 4.5, 5.5],\n",
       "  [4, 4, 4, None, None, 4, 4],\n",
       "  [4.5, 4, 4, None, None, 4, 4.5],\n",
       "  [4.5, 4.5, 4.5, None, None, 4.5, 4.5],\n",
       "  [5, 4, 4.5, None, None, 4.5, 5],\n",
       "  [5.5, 5, 5, None, None, 5, 5.5],\n",
       "  [5.5, 4.5, 5, None, None, 5, 5.5],\n",
       "  [3.5, 3.5, 3.5, None, None, 3.5, 3.5],\n",
       "  [5, 4, 4, None, None, 4, 4.5],\n",
       "  [5.5, 4, 5.5, 5, 5, 5, 5.5],\n",
       "  [4, 4.5, 4, 3.5, 3.5, 4, 4],\n",
       "  [3, 4.5, 4, 4, 3.5, 4, 3.5],\n",
       "  [4.5, 5, 3.5, 4, 5, 4.5, 4.5],\n",
       "  [4, 4.5, 4, 4, 4.5, 4.5, 4.5],\n",
       "  [5, 3.5, 5, 5, 4.5, 4.5, 5],\n",
       "  [4, 4, 5, 4, 4, 4.5, 4.5],\n",
       "  [4, 3, 4, 3, 4, 3.5, 4],\n",
       "  [4.5, 4.5, 4, 4, 4.5, 4.5, 4.5],\n",
       "  [3.5, 3.5, 2.5, 3.5, 2.5, 3, 3.5],\n",
       "  [4.5, 3, 4, 3.5, 5, 4, 4.5],\n",
       "  [3, 3.5, 2.5, 3, 4, 3.5, 3.5],\n",
       "  [3.5, 4.5, 4.5, 4, 5.5, 4.5, 4],\n",
       "  [4, 3.5, 3.5, 4, 4.5, 4, 4],\n",
       "  [3, 4.5, 4.5, 4, 3, 4, 3.5],\n",
       "  [4.5, 4, 4.5, 4.5, 5, 4.5, 4.5],\n",
       "  [4.5, 4, 4, 5, 4.5, 4.5, 4.5],\n",
       "  [4, 4, 4, 4, 5.5, 4.5, 4.5],\n",
       "  [4.5, 4, 4, 4, 5, 4.5, 4.5],\n",
       "  [4, 4.5, 4.5, 4.5, 5, 4.5, 4.5],\n",
       "  [5, 5, 4.5, 4, 4.5, 4.5, 5],\n",
       "  [3.5, 4.5, 4, 3.5, 4, 4, 4],\n",
       "  [5, 4, 3, 4, 4.5, 4, 4.5],\n",
       "  [4, 4, 4, 4, 4, 4, 4],\n",
       "  [4, 4, 4.5, 4, 4, 4, 4],\n",
       "  [2.5, 5, 4.5, 4, 4, 4.5, 3.5],\n",
       "  [5, 5, 5, 4, 4.5, 4.5, 5],\n",
       "  [4, 5.5, 4.5, 5, 4.5, 5, 4.5],\n",
       "  [2, 4, 3.5, 3, 3, 3.5, 3],\n",
       "  [5.5, 6, 6, 5.5, 5.5, 6, 6],\n",
       "  [3, 3.5, 3, None, None, 3.5, 3.5],\n",
       "  [3, 3.5, 3, None, None, 3.5, 3.5],\n",
       "  [3, 4, 3.5, None, None, 4, 3.5],\n",
       "  [2.5, 3.5, 3.5, None, None, 3.5, 3],\n",
       "  [2, 2.5, 2, None, None, 2.5, 2.5],\n",
       "  [1.5, 2.5, 2, None, None, 2.5, 2],\n",
       "  [2.5, 4, 3.5, None, None, 4, 3.5],\n",
       "  [3.5, 4.5, 3.5, None, None, 4, 4],\n",
       "  [3.5, 4.5, 3.5, None, None, 4, 4],\n",
       "  [2.5, 3.5, 4, None, None, 4, 3.5],\n",
       "  [4, 5.5, 5, None, None, 5.5, 5],\n",
       "  [5, 5, 5, None, None, 5, 5],\n",
       "  [5, 5, 5, 5, 5, 5, 5],\n",
       "  [5, 4.5, 4.5, 4.5, 4.5, 4.5, 5],\n",
       "  [3.5, 4, 4, 4.5, 4, 4, 4],\n",
       "  [5, 4.5, 4, 4.5, 4, 4.5, 5],\n",
       "  [3.5, 4, 4, 3.5, 4, 4, 4],\n",
       "  [4, None, 5, 4.5, 4.5, 4.5, 4.5],\n",
       "  [2.5, 4.5, 4.5, 4.5, 3.5, 4.5, 3.5],\n",
       "  [3.5, 4.5, 5, 4, 5, 4.5, 4],\n",
       "  [4.5, 5, 5.5, 4.5, 5, 5, 5],\n",
       "  [5, 4.5, 5, 5, 5.5, 5, 5],\n",
       "  [5, 4, 4.5, 4.5, 4.5, 4.5, 5],\n",
       "  [4.5, 4, 4.5, 4.5, 4, 4.5, 4.5],\n",
       "  [2.5, 3.5, 3.5, 3.5, 3, 3.5, 3],\n",
       "  [3.5, 4.5, 4, 3.5, 3.5, 4, 4],\n",
       "  [5.5, 5, 5.5, 5.5, 5.5, 5.5, 5.5],\n",
       "  [2.5, 4, 4, 4, 4.5, 4, 3.5],\n",
       "  [4, 3.5, 3, 2.5, 3.5, 3, 3.5],\n",
       "  [5, 5.5, 5.5, 5.5, 3.5, 5, 5],\n",
       "  [4.5, 4, 4, 3.5, 5, 4, 4.5],\n",
       "  [4.5, 5, 5, 5.5, 5, 5, 5],\n",
       "  [4, 4, 4.5, 4.5, 3.5, 4, 4],\n",
       "  [3, 5, 3, 3.5, 3.5, 4, 3.5],\n",
       "  [4.5, 5.5, 5.5, 5, 5, 5.5, 5],\n",
       "  [6, 6, 6, 6, 5.5, 6, 6],\n",
       "  [3.5, 3.5, 3, 3, 3, 3, 3.5],\n",
       "  [4, 4, 4.5, 4, 4, 4, 4],\n",
       "  [4.5, 4.5, 5, 4.5, 4.5, 4.5, 4.5],\n",
       "  [4, 3, 4, 4, 4.5, 4, 4],\n",
       "  [3.5, 4.5, 4, 3.5, 3.5, 4, 4],\n",
       "  [2.5, 2.5, 3, 2.5, 3, 3, 3],\n",
       "  [4, 4, 5, 4, 4, 4.5, 4.5],\n",
       "  [4.5, 3.5, 5, 4.5, 4, 4.5, 4.5],\n",
       "  [5, 5, 4.5, 4, 3.5, 4.5, 5],\n",
       "  [4.5, 4, 4, 3.5, 3.5, 4, 4.5],\n",
       "  [4, 3.5, 4, 4, 3.5, 4, 4],\n",
       "  [4, 4, 4.5, 4.5, 4.5, 4.5, 4.5],\n",
       "  [4.5, 4.5, 4, 4, 4.5, 4.5, 4.5],\n",
       "  [4.5, 4.5, 5, 4.5, 3.5, 4.5, 4.5],\n",
       "  [3.5, 4.5, 5, None, None, 5, 4.5],\n",
       "  [2.5, 4, 4, None, None, 4, 3.5],\n",
       "  [5, 4.5, 4.5, None, None, 4.5, 5],\n",
       "  [2, 3, 3, None, None, 3, 2.5],\n",
       "  [3.5, 4, 4, None, None, 4, 4],\n",
       "  [3.5, 5, 4.5, None, None, 5, 4.5],\n",
       "  [2, 4, 3.5, None, None, 4, 3],\n",
       "  [2, 2.5, 2.5, None, None, 2.5, 2.5]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(data).shape, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db4185cf-5c5a-4472-82d7-40c7d2e445be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((138, 7),\n",
       " [[4, 4, 4, 3.5, 4, 4, 4],\n",
       "  [4.5, 4.5, 5.5, 4.5, 4.5, 5, 5],\n",
       "  [1.5, 2.5, 3.5, 3, 2, 3, 2.5],\n",
       "  [4.5, 5, 5, 5, 5, 5, 5],\n",
       "  [3.5, 4.5, 4.5, 4, 4.5, 4.5, 4],\n",
       "  [4, 5.5, 5.5, 4.5, 5.5, 5.5, 5],\n",
       "  [4.5, 4, 4, 3.5, 3.5, 4, 4.5],\n",
       "  [2.5, 4, 4, 4, 4, 4, 3.5],\n",
       "  [5, 5, 5, 5, 4.5, 5, 5],\n",
       "  [6, 5.5, 5.5, 4.5, 6, 5.5, 6],\n",
       "  [4.5, 4.5, 4.5, 3, 5, 4.5, 4.5],\n",
       "  [4, 5, 5, 4, 5, 5, 4.5],\n",
       "  [4, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5],\n",
       "  [3, 4, 4, 2.5, 3, 3.5, 3.5],\n",
       "  [2, 3.5, 3.5, 2.5, 2.5, 3, 2.5],\n",
       "  [3, 4, 4, 3.5, 3.5, 4, 3.5],\n",
       "  [2, 4.5, 4.5, 3, 4, 4, 3],\n",
       "  [2.5, 3.5, 3.5, 3.5, 3.5, 3.5, 3],\n",
       "  [4, 4, 4.5, 3.5, 3.5, 4, 4],\n",
       "  [4, 4.5, 4.5, 4, 5.5, 4.5, 4.5],\n",
       "  [4, 5.5, 5.5, 5.5, 4.5, 5.5, 5],\n",
       "  [3.5, 4, 4, 4, 4, 4, 4],\n",
       "  [3.5, 4, 4.5, 4.5, 4, 4.5, 4],\n",
       "  [3, 3.5, 4, 3.5, 3.5, 3.5, 3.5],\n",
       "  [4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5],\n",
       "  [5.5, 5, 5, 5.5, 4.5, 5, 5.5],\n",
       "  [5.5, 5.5, 4.5, 5, 4.5, 5, 5.5],\n",
       "  [4.5, 4, 4.5, 3.5, 4, 4, 4.5],\n",
       "  [5.5, 5, 5, 5.5, 5, 5, 5.5],\n",
       "  [6, 5, 5.5, 5, 4.5, 5, 5.5],\n",
       "  [5, 5.5, 5.5, 5.5, 5, 5.5, 5.5],\n",
       "  [5, 4.5, 5, 5, 4.5, 5, 5],\n",
       "  [4, 4.5, 5, 4.5, 4, 4.5, 4.5],\n",
       "  [4, 4, 4, 3.5, 3.5, 4, 4],\n",
       "  [3, 4.5, 4, 4, 4, 4, 3.5],\n",
       "  [4.5, 5, 5.5, 4.5, 3.5, 4.5, 4.5],\n",
       "  [6, 5.5, 5, 5.5, 5, 5.5, 6],\n",
       "  [6, 5.5, 5, 5.5, 5, 5.5, 6],\n",
       "  [4, 4, 3, 3, 3, 3.5, 4],\n",
       "  [4, 4, 4, 4.5, 3.5, 4, 4],\n",
       "  [5, 4, 4, 4.5, 4.5, 4.5, 5],\n",
       "  [4, 4.5, 4.5, 4, 3.5, 4, 4],\n",
       "  [4, 4.5, 3.5, 3.5, 3.5, 4, 4],\n",
       "  [4.5, 4, 4, 3.5, 4.5, 4, 4.5],\n",
       "  [5, 4, 3, 4.5, 4, 4, 4.5],\n",
       "  [3.5, 4.5, 4.5, 4, 3.5, 4, 4],\n",
       "  [3, 4.5, 4.5, 4.5, 3.5, 4.5, 4],\n",
       "  [4, 3.5, 4, 3.5, 4, 4, 4],\n",
       "  [3.5, 4, 4, 4, 4, 4, 4],\n",
       "  [3.5, 5, 4, 4, 4, 4.5, 4],\n",
       "  [4, 4.5, 4, 4.5, 4, 4.5, 4.5],\n",
       "  [4, 5, 4.5, 5.5, 4, 5, 4.5],\n",
       "  [4, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5],\n",
       "  [3.5, 4.5, 3.5, 4, 4, 4, 4],\n",
       "  [4, 4.5, 4, 4, 4.5, 4.5, 4.5],\n",
       "  [3.5, 4.5, 4, 4.5, 4, 4.5, 4],\n",
       "  [3.5, 4, 3.5, 3, 3.5, 3.5, 3.5],\n",
       "  [3, 3, 3.5, 3, 4, 3.5, 3.5],\n",
       "  [4.5, 5, 5.5, 5.5, 6, 5.5, 5],\n",
       "  [5, 4, 4.5, 5.5, 3.5, 4.5, 5],\n",
       "  [3.5, 3.5, 3.5, 4, 3.5, 3.5, 3.5],\n",
       "  [5, 5, 5.5, 4.5, 4, 5, 5],\n",
       "  [4.5, 4.5, 4, 4.5, 4, 4.5, 4.5],\n",
       "  [5, 4.5, 5, 4.5, 4.5, 4.5, 5],\n",
       "  [4.5, 5, 4.5, 4.5, 5, 5, 5],\n",
       "  [3, 4, 3.5, 3.5, 3, 3.5, 3.5],\n",
       "  [4, 4.5, 3.5, 3.5, 3.5, 4, 4],\n",
       "  [1.5, 2.5, 2.5, 2, 1.5, 2, 2],\n",
       "  [3.5, 5, 4.5, 4.5, 5, 5, 4.5],\n",
       "  [5, 5, 5, 5, 4.5, 5, 5],\n",
       "  [4.5, 4.5, 3.5, 5, 5, 4.5, 4.5],\n",
       "  [5.5, 4, 5.5, 5, 5, 5, 5.5],\n",
       "  [4, 4.5, 4, 3.5, 3.5, 4, 4],\n",
       "  [3, 4.5, 4, 4, 3.5, 4, 3.5],\n",
       "  [4.5, 5, 3.5, 4, 5, 4.5, 4.5],\n",
       "  [4, 4.5, 4, 4, 4.5, 4.5, 4.5],\n",
       "  [5, 3.5, 5, 5, 4.5, 4.5, 5],\n",
       "  [4, 4, 5, 4, 4, 4.5, 4.5],\n",
       "  [4, 3, 4, 3, 4, 3.5, 4],\n",
       "  [4.5, 4.5, 4, 4, 4.5, 4.5, 4.5],\n",
       "  [3.5, 3.5, 2.5, 3.5, 2.5, 3, 3.5],\n",
       "  [4.5, 3, 4, 3.5, 5, 4, 4.5],\n",
       "  [3, 3.5, 2.5, 3, 4, 3.5, 3.5],\n",
       "  [3.5, 4.5, 4.5, 4, 5.5, 4.5, 4],\n",
       "  [4, 3.5, 3.5, 4, 4.5, 4, 4],\n",
       "  [3, 4.5, 4.5, 4, 3, 4, 3.5],\n",
       "  [4.5, 4, 4.5, 4.5, 5, 4.5, 4.5],\n",
       "  [4.5, 4, 4, 5, 4.5, 4.5, 4.5],\n",
       "  [4, 4, 4, 4, 5.5, 4.5, 4.5],\n",
       "  [4.5, 4, 4, 4, 5, 4.5, 4.5],\n",
       "  [4, 4.5, 4.5, 4.5, 5, 4.5, 4.5],\n",
       "  [5, 5, 4.5, 4, 4.5, 4.5, 5],\n",
       "  [3.5, 4.5, 4, 3.5, 4, 4, 4],\n",
       "  [5, 4, 3, 4, 4.5, 4, 4.5],\n",
       "  [4, 4, 4, 4, 4, 4, 4],\n",
       "  [4, 4, 4.5, 4, 4, 4, 4],\n",
       "  [2.5, 5, 4.5, 4, 4, 4.5, 3.5],\n",
       "  [5, 5, 5, 4, 4.5, 4.5, 5],\n",
       "  [4, 5.5, 4.5, 5, 4.5, 5, 4.5],\n",
       "  [2, 4, 3.5, 3, 3, 3.5, 3],\n",
       "  [5.5, 6, 6, 5.5, 5.5, 6, 6],\n",
       "  [5, 5, 5, 5, 5, 5, 5],\n",
       "  [5, 4.5, 4.5, 4.5, 4.5, 4.5, 5],\n",
       "  [3.5, 4, 4, 4.5, 4, 4, 4],\n",
       "  [5, 4.5, 4, 4.5, 4, 4.5, 5],\n",
       "  [3.5, 4, 4, 3.5, 4, 4, 4],\n",
       "  [2.5, 4.5, 4.5, 4.5, 3.5, 4.5, 3.5],\n",
       "  [3.5, 4.5, 5, 4, 5, 4.5, 4],\n",
       "  [4.5, 5, 5.5, 4.5, 5, 5, 5],\n",
       "  [5, 4.5, 5, 5, 5.5, 5, 5],\n",
       "  [5, 4, 4.5, 4.5, 4.5, 4.5, 5],\n",
       "  [4.5, 4, 4.5, 4.5, 4, 4.5, 4.5],\n",
       "  [2.5, 3.5, 3.5, 3.5, 3, 3.5, 3],\n",
       "  [3.5, 4.5, 4, 3.5, 3.5, 4, 4],\n",
       "  [5.5, 5, 5.5, 5.5, 5.5, 5.5, 5.5],\n",
       "  [2.5, 4, 4, 4, 4.5, 4, 3.5],\n",
       "  [4, 3.5, 3, 2.5, 3.5, 3, 3.5],\n",
       "  [5, 5.5, 5.5, 5.5, 3.5, 5, 5],\n",
       "  [4.5, 4, 4, 3.5, 5, 4, 4.5],\n",
       "  [4.5, 5, 5, 5.5, 5, 5, 5],\n",
       "  [4, 4, 4.5, 4.5, 3.5, 4, 4],\n",
       "  [3, 5, 3, 3.5, 3.5, 4, 3.5],\n",
       "  [4.5, 5.5, 5.5, 5, 5, 5.5, 5],\n",
       "  [6, 6, 6, 6, 5.5, 6, 6],\n",
       "  [3.5, 3.5, 3, 3, 3, 3, 3.5],\n",
       "  [4, 4, 4.5, 4, 4, 4, 4],\n",
       "  [4.5, 4.5, 5, 4.5, 4.5, 4.5, 4.5],\n",
       "  [4, 3, 4, 4, 4.5, 4, 4],\n",
       "  [3.5, 4.5, 4, 3.5, 3.5, 4, 4],\n",
       "  [2.5, 2.5, 3, 2.5, 3, 3, 3],\n",
       "  [4, 4, 5, 4, 4, 4.5, 4.5],\n",
       "  [4.5, 3.5, 5, 4.5, 4, 4.5, 4.5],\n",
       "  [5, 5, 4.5, 4, 3.5, 4.5, 5],\n",
       "  [4.5, 4, 4, 3.5, 3.5, 4, 4.5],\n",
       "  [4, 3.5, 4, 4, 3.5, 4, 4],\n",
       "  [4, 4, 4.5, 4.5, 4.5, 4.5, 4.5],\n",
       "  [4.5, 4.5, 4, 4, 4.5, 4.5, 4.5],\n",
       "  [4.5, 4.5, 5, 4.5, 3.5, 4.5, 4.5]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filtered = [d for d in data if not None in d]\n",
    "#data_filtered = data\n",
    "\n",
    "np.array(data_filtered).shape, data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e5b9323-8460-4444-9935-306050158534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((138, 4),\n",
       " array([[4. , 4. , 3.5, 4. ],\n",
       "        [4.5, 5.5, 4.5, 4.5],\n",
       "        [2.5, 3.5, 3. , 2. ],\n",
       "        [5. , 5. , 5. , 5. ],\n",
       "        [4.5, 4.5, 4. , 4.5],\n",
       "        [5.5, 5.5, 4.5, 5.5],\n",
       "        [4. , 4. , 3.5, 3.5],\n",
       "        [4. , 4. , 4. , 4. ],\n",
       "        [5. , 5. , 5. , 4.5],\n",
       "        [5.5, 5.5, 4.5, 6. ],\n",
       "        [4.5, 4.5, 3. , 5. ],\n",
       "        [5. , 5. , 4. , 5. ],\n",
       "        [4.5, 4.5, 4.5, 4.5],\n",
       "        [4. , 4. , 2.5, 3. ],\n",
       "        [3.5, 3.5, 2.5, 2.5],\n",
       "        [4. , 4. , 3.5, 3.5],\n",
       "        [4.5, 4.5, 3. , 4. ],\n",
       "        [3.5, 3.5, 3.5, 3.5],\n",
       "        [4. , 4.5, 3.5, 3.5],\n",
       "        [4.5, 4.5, 4. , 5.5],\n",
       "        [5.5, 5.5, 5.5, 4.5],\n",
       "        [4. , 4. , 4. , 4. ],\n",
       "        [4. , 4.5, 4.5, 4. ],\n",
       "        [3.5, 4. , 3.5, 3.5],\n",
       "        [4.5, 4.5, 4.5, 4.5],\n",
       "        [5. , 5. , 5.5, 4.5],\n",
       "        [5.5, 4.5, 5. , 4.5],\n",
       "        [4. , 4.5, 3.5, 4. ],\n",
       "        [5. , 5. , 5.5, 5. ],\n",
       "        [5. , 5.5, 5. , 4.5],\n",
       "        [5.5, 5.5, 5.5, 5. ],\n",
       "        [4.5, 5. , 5. , 4.5],\n",
       "        [4.5, 5. , 4.5, 4. ],\n",
       "        [4. , 4. , 3.5, 3.5],\n",
       "        [4.5, 4. , 4. , 4. ],\n",
       "        [5. , 5.5, 4.5, 3.5],\n",
       "        [5.5, 5. , 5.5, 5. ],\n",
       "        [5.5, 5. , 5.5, 5. ],\n",
       "        [4. , 3. , 3. , 3. ],\n",
       "        [4. , 4. , 4.5, 3.5],\n",
       "        [4. , 4. , 4.5, 4.5],\n",
       "        [4.5, 4.5, 4. , 3.5],\n",
       "        [4.5, 3.5, 3.5, 3.5],\n",
       "        [4. , 4. , 3.5, 4.5],\n",
       "        [4. , 3. , 4.5, 4. ],\n",
       "        [4.5, 4.5, 4. , 3.5],\n",
       "        [4.5, 4.5, 4.5, 3.5],\n",
       "        [3.5, 4. , 3.5, 4. ],\n",
       "        [4. , 4. , 4. , 4. ],\n",
       "        [5. , 4. , 4. , 4. ],\n",
       "        [4.5, 4. , 4.5, 4. ],\n",
       "        [5. , 4.5, 5.5, 4. ],\n",
       "        [4.5, 4.5, 4.5, 4.5],\n",
       "        [4.5, 3.5, 4. , 4. ],\n",
       "        [4.5, 4. , 4. , 4.5],\n",
       "        [4.5, 4. , 4.5, 4. ],\n",
       "        [4. , 3.5, 3. , 3.5],\n",
       "        [3. , 3.5, 3. , 4. ],\n",
       "        [5. , 5.5, 5.5, 6. ],\n",
       "        [4. , 4.5, 5.5, 3.5],\n",
       "        [3.5, 3.5, 4. , 3.5],\n",
       "        [5. , 5.5, 4.5, 4. ],\n",
       "        [4.5, 4. , 4.5, 4. ],\n",
       "        [4.5, 5. , 4.5, 4.5],\n",
       "        [5. , 4.5, 4.5, 5. ],\n",
       "        [4. , 3.5, 3.5, 3. ],\n",
       "        [4.5, 3.5, 3.5, 3.5],\n",
       "        [2.5, 2.5, 2. , 1.5],\n",
       "        [5. , 4.5, 4.5, 5. ],\n",
       "        [5. , 5. , 5. , 4.5],\n",
       "        [4.5, 3.5, 5. , 5. ],\n",
       "        [4. , 5.5, 5. , 5. ],\n",
       "        [4.5, 4. , 3.5, 3.5],\n",
       "        [4.5, 4. , 4. , 3.5],\n",
       "        [5. , 3.5, 4. , 5. ],\n",
       "        [4.5, 4. , 4. , 4.5],\n",
       "        [3.5, 5. , 5. , 4.5],\n",
       "        [4. , 5. , 4. , 4. ],\n",
       "        [3. , 4. , 3. , 4. ],\n",
       "        [4.5, 4. , 4. , 4.5],\n",
       "        [3.5, 2.5, 3.5, 2.5],\n",
       "        [3. , 4. , 3.5, 5. ],\n",
       "        [3.5, 2.5, 3. , 4. ],\n",
       "        [4.5, 4.5, 4. , 5.5],\n",
       "        [3.5, 3.5, 4. , 4.5],\n",
       "        [4.5, 4.5, 4. , 3. ],\n",
       "        [4. , 4.5, 4.5, 5. ],\n",
       "        [4. , 4. , 5. , 4.5],\n",
       "        [4. , 4. , 4. , 5.5],\n",
       "        [4. , 4. , 4. , 5. ],\n",
       "        [4.5, 4.5, 4.5, 5. ],\n",
       "        [5. , 4.5, 4. , 4.5],\n",
       "        [4.5, 4. , 3.5, 4. ],\n",
       "        [4. , 3. , 4. , 4.5],\n",
       "        [4. , 4. , 4. , 4. ],\n",
       "        [4. , 4.5, 4. , 4. ],\n",
       "        [5. , 4.5, 4. , 4. ],\n",
       "        [5. , 5. , 4. , 4.5],\n",
       "        [5.5, 4.5, 5. , 4.5],\n",
       "        [4. , 3.5, 3. , 3. ],\n",
       "        [6. , 6. , 5.5, 5.5],\n",
       "        [5. , 5. , 5. , 5. ],\n",
       "        [4.5, 4.5, 4.5, 4.5],\n",
       "        [4. , 4. , 4.5, 4. ],\n",
       "        [4.5, 4. , 4.5, 4. ],\n",
       "        [4. , 4. , 3.5, 4. ],\n",
       "        [4.5, 4.5, 4.5, 3.5],\n",
       "        [4.5, 5. , 4. , 5. ],\n",
       "        [5. , 5.5, 4.5, 5. ],\n",
       "        [4.5, 5. , 5. , 5.5],\n",
       "        [4. , 4.5, 4.5, 4.5],\n",
       "        [4. , 4.5, 4.5, 4. ],\n",
       "        [3.5, 3.5, 3.5, 3. ],\n",
       "        [4.5, 4. , 3.5, 3.5],\n",
       "        [5. , 5.5, 5.5, 5.5],\n",
       "        [4. , 4. , 4. , 4.5],\n",
       "        [3.5, 3. , 2.5, 3.5],\n",
       "        [5.5, 5.5, 5.5, 3.5],\n",
       "        [4. , 4. , 3.5, 5. ],\n",
       "        [5. , 5. , 5.5, 5. ],\n",
       "        [4. , 4.5, 4.5, 3.5],\n",
       "        [5. , 3. , 3.5, 3.5],\n",
       "        [5.5, 5.5, 5. , 5. ],\n",
       "        [6. , 6. , 6. , 5.5],\n",
       "        [3.5, 3. , 3. , 3. ],\n",
       "        [4. , 4.5, 4. , 4. ],\n",
       "        [4.5, 5. , 4.5, 4.5],\n",
       "        [3. , 4. , 4. , 4.5],\n",
       "        [4.5, 4. , 3.5, 3.5],\n",
       "        [2.5, 3. , 2.5, 3. ],\n",
       "        [4. , 5. , 4. , 4. ],\n",
       "        [3.5, 5. , 4.5, 4. ],\n",
       "        [5. , 4.5, 4. , 3.5],\n",
       "        [4. , 4. , 3.5, 3.5],\n",
       "        [3.5, 4. , 4. , 3.5],\n",
       "        [4. , 4.5, 4.5, 4.5],\n",
       "        [4.5, 4. , 4. , 4.5],\n",
       "        [4.5, 5. , 4.5, 3.5]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filtered_array = np.array(data_filtered)\n",
    "pnote = data_filtered_array[:,0]\n",
    "enote = data_filtered_array[:,-2]\n",
    "fnote = data_filtered_array[:,-1]\n",
    "noten = data_filtered_array[:,1:-2]\n",
    "\n",
    "noten.shape, noten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dcd1362a-a130-4ed1-8b9e-dfeb650b2803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "L = 4 + 1\n",
    "N = 138\n",
    "\n",
    "torch.save(np.c_[ noten, fnote ], open('traindata_noten.pt', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bb1588b2-1fda-4027-8012-ed467e967987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:  0\n",
      "loss: 18.20606713528468\n",
      "loss: 18.15393352351447\n",
      "loss: 14.104327052537734\n",
      "loss: 5.663230158070774\n",
      "loss: 3.634658484874548\n",
      "loss: 2.8961572522160983\n",
      "loss: 2.385924354755726\n",
      "loss: 2.0004138429506404\n",
      "loss: 1.712873218125267\n",
      "loss: 1.5021565763880012\n",
      "loss: 1.3483801358698737\n",
      "loss: 1.234976779263606\n",
      "loss: 1.1497083709894536\n",
      "loss: 1.0841092694205912\n",
      "loss: 1.0324385417578739\n",
      "loss: 0.990780876451845\n",
      "loss: 0.9564208410023481\n",
      "loss: 0.9274354463987762\n",
      "loss: 0.9024319685933052\n",
      "loss: 0.8803774256355672\n",
      "test loss: 1.249672746533715\n",
      "STEP:  1\n",
      "loss: 0.8604853092280598\n",
      "loss: 0.8421381638146126\n",
      "loss: 0.8248325241072655\n",
      "loss: 0.8081372708684145\n",
      "loss: 0.7916587571038739\n",
      "loss: 0.7750065213005181\n",
      "loss: 0.7577516648317277\n",
      "loss: 0.7393641600607821\n",
      "loss: 0.7190997050043586\n",
      "loss: 0.6957634605154804\n",
      "loss: 0.667150943322609\n",
      "loss: 0.6286328791002403\n",
      "loss: 0.5713200494716832\n",
      "loss: 0.5151083225461247\n",
      "loss: 0.5028260392267145\n",
      "loss: 0.4951205912177548\n",
      "loss: 0.4856711814140645\n",
      "loss: 0.47382261023259864\n",
      "loss: 0.4597530712130526\n",
      "loss: 0.4416055619220464\n",
      "test loss: 0.7855986585370386\n",
      "STEP:  2\n",
      "loss: 0.41937848991384286\n",
      "loss: 0.3956447673643675\n",
      "loss: 0.3732325924014181\n",
      "loss: 0.35319452739072593\n",
      "loss: 0.334187729079115\n",
      "loss: 0.31519989614659666\n",
      "loss: 0.3031512268795627\n",
      "loss: 0.2947924941569431\n",
      "loss: 0.28908754462941955\n",
      "loss: 0.2857762710726378\n",
      "loss: 0.283389087543179\n",
      "loss: 0.281542541087294\n",
      "loss: 0.2800181463442421\n",
      "loss: 0.2787209877694563\n",
      "loss: 0.27756558331480236\n",
      "loss: 0.2764705510592372\n",
      "loss: 0.27538431852863776\n",
      "loss: 0.27433619421199584\n",
      "loss: 0.2733342937846536\n",
      "loss: 0.2724772145558385\n",
      "test loss: 0.27422406610801014\n",
      "STEP:  3\n",
      "loss: 0.2716135830314964\n",
      "loss: 0.2707609003451246\n",
      "loss: 0.2698684448921647\n",
      "loss: 0.26867420383562657\n",
      "loss: 0.2672867740862503\n",
      "loss: 0.2663147482906866\n",
      "loss: 0.26539471851084634\n",
      "loss: 0.2647151510509124\n",
      "loss: 0.26400273243595057\n",
      "loss: 0.2634850675404595\n",
      "loss: 0.26301983668944606\n",
      "loss: 0.262572397378022\n",
      "loss: 0.26214736606805106\n",
      "loss: 0.2617397864187645\n",
      "loss: 0.26136101568072917\n",
      "loss: 0.2610072688154231\n",
      "loss: 0.2606809179145446\n",
      "loss: 0.2603744867611974\n",
      "loss: 0.26009089592056234\n",
      "loss: 0.2598247915788598\n",
      "test loss: 0.22780979379015875\n",
      "STEP:  4\n",
      "loss: 0.25957342689846646\n",
      "loss: 0.25934171616986396\n",
      "loss: 0.259120586719967\n",
      "loss: 0.2589225898741236\n",
      "loss: 0.25874012731123636\n",
      "loss: 0.2585750046556481\n",
      "loss: 0.2584272297575153\n",
      "loss: 0.2582921793596121\n",
      "loss: 0.25817247509599184\n",
      "loss: 0.25806162639068253\n",
      "loss: 0.2579620381651878\n",
      "loss: 0.2578677140357181\n",
      "loss: 0.25778089346079164\n",
      "loss: 0.25769680742473555\n",
      "loss: 0.2576172971985928\n",
      "loss: 0.25753887047020496\n",
      "loss: 0.2574627133287814\n",
      "loss: 0.25738644800400695\n",
      "loss: 0.257310480038534\n",
      "loss: 0.2572334332503614\n",
      "test loss: 0.25345117809733264\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Sequence(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Sequence, self).__init__()\n",
    "        self.lstm1 = nn.LSTMCell(1, 51)\n",
    "        self.lstm2 = nn.LSTMCell(51, 51)\n",
    "        self.linear = nn.Linear(51, 1)\n",
    "\n",
    "    def forward(self, input, future = 0):\n",
    "        outputs = []\n",
    "        h_t = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
    "        c_t = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
    "        h_t2 = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
    "        c_t2 = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
    "\n",
    "        for input_t in input.split(1, dim=1):\n",
    "            h_t, c_t = self.lstm1(input_t, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
    "            output = self.linear(h_t2)\n",
    "            outputs += [output]\n",
    "        for i in range(future):# if we should predict the future\n",
    "            h_t, c_t = self.lstm1(output, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
    "            output = self.linear(h_t2)\n",
    "            outputs += [output]\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--steps', type=int, default=15, help='steps to run')\n",
    "#    opt = parser.parse_args()\n",
    "    STEPS = 5\n",
    "    # set random seed to 0\n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    # load data and make training set\n",
    "    data = torch.load('traindata_noten.pt')\n",
    "    input = torch.from_numpy(data[3:, :-1])\n",
    "    target = torch.from_numpy(data[3:, 1:])\n",
    "    test_input = torch.from_numpy(data[:3, :-1])\n",
    "    test_target = torch.from_numpy(data[:3, 1:])\n",
    "    # build the model\n",
    "    seq = Sequence()\n",
    "    seq.double()\n",
    "    criterion = nn.MSELoss()\n",
    "    # use LBFGS as optimizer since we can load the whole data to train\n",
    "    optimizer = optim.LBFGS(seq.parameters(), lr=0.08)\n",
    "    #begin to train\n",
    "#    for i in range(opt.steps):\n",
    "    for i in range(STEPS):\n",
    "        print('STEP: ', i)\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            out = seq(input)\n",
    "            loss = criterion(out, target)\n",
    "            print('loss:', loss.item())\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        optimizer.step(closure)\n",
    "        # begin to predict, no need to track gradient here\n",
    "        with torch.no_grad():\n",
    "            future = 1\n",
    "            pred = seq(test_input, future=future)\n",
    "            loss = criterion(pred[:, :-future], test_target)\n",
    "            print('test loss:', loss.item())\n",
    "            y = pred.detach().numpy()\n",
    "        # draw the result\n",
    "        plt.figure(figsize=(30,10))\n",
    "        plt.title('Predict future values for time sequences\\n(Dashlines are predicted values)', fontsize=30)\n",
    "        plt.xlabel('x', fontsize=20)\n",
    "        plt.ylabel('y', fontsize=20)\n",
    "        plt.xticks(fontsize=20)\n",
    "        plt.yticks(fontsize=20)\n",
    "        def draw(yi, color):\n",
    "            plt.plot(np.arange(input.size(1)), yi[:input.size(1)], color + 'o', linewidth = 2.0)\n",
    "            plt.plot(np.arange(input.size(1), input.size(1) + future), yi[input.size(1):], color + 'x', linewidth = 2.0)\n",
    "        draw(y[0], 'r')\n",
    "        draw(y[1], 'g')\n",
    "        draw(y[2], 'b')\n",
    "        plt.savefig('predict%d_noten.pdf'%i)\n",
    "        plt.close()\n",
    "    # https://stackoverflow.com/questions/42703500/how-do-i-save-a-trained-model-in-pytorch\n",
    "    torch.save(seq, \"modeldata_noten.pt\")\n",
    "    #torch.save(seq.state_dict(), \"modelstate_noten.pt\")\n",
    "\n",
    "# zufällige 3 reihen auswählen als \"test_data\"\n",
    "# google; \"exam mark ai\" -> literatur suchen\n",
    "# ev. model selbst anpassen; anzahl lstm layer, anzahl hidden_states (freiheitsgrade)\n",
    "# trainieren auf pnote statt fnote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6f916061-9454-4ec4-b4bc-b9f2c53ef3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.25345117809733264\n"
     ]
    }
   ],
   "source": [
    "seq = torch.load(\"modeldata_noten.pt\")\n",
    "#seq.load_state_dict(torch.load(\"modelstate_noten.pt\"))\n",
    "\n",
    "# begin to predict, no need to track gradient here\n",
    "with torch.no_grad():\n",
    "    future = 1\n",
    "    pred = seq(test_input, future=future)\n",
    "    loss = criterion(pred[:, :-future], test_target)\n",
    "    print('test loss:', loss.item())\n",
    "    y = pred.detach().numpy()\n",
    "# draw the result\n",
    "plt.figure(figsize=(30,10))\n",
    "plt.title('Predict future values for time sequences\\n(Dashlines are predicted values)', fontsize=30)\n",
    "plt.xlabel('x', fontsize=20)\n",
    "plt.ylabel('y', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "def draw(yi, color):\n",
    "    plt.plot(np.arange(input.size(1)), yi[:input.size(1)], color + 'o', linewidth = 2.0)\n",
    "    plt.plot(np.arange(input.size(1), input.size(1) + future), yi[input.size(1):], color + 'x', linewidth = 2.0)\n",
    "draw(y[0], 'r')\n",
    "draw(y[1], 'g')\n",
    "draw(y[2], 'b')\n",
    "plt.savefig('predict_noten.pdf')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e800b154-e5d5-4fe8-98d0-2129d5f8998c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
